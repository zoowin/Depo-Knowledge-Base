import requests
import json
import os
import time
import random
import html
from datetime import datetime

# --- Configuration ---
# æ ¸å¿ƒç§å­è¯ï¼šåŸºäº Depology çš„äº§å“çº¿
SEED_KEYWORDS = [
    "matrixyl 3000", 
    "argireline solution", 
    "micro-dart patches", 
    "retinol for sensitive skin", 
    "cica redness relief",
    "peptide serum benefits"
]

# ç›®æ ‡ Subreddits
SUBREDDITS = ["SkincareAddiction", "30PlusSkinCare", "AsianBeauty", "DermatologyQuestions"]

# ç›®æ ‡æ–‡ä»¶
TARGET_FILE = r"..\Shopify SEO Blog\Blog_Topic_Pool.md"

def fetch_google_suggestions(keyword):
    """
    è·å– Google æœç´¢ä¸‹æ‹‰æ¨èè¯ (Long-tail Keywords)
    """
    print(f"ğŸ” [Google] Searching suggestions for '{keyword}'...")
    url = f"http://google.com/complete/search?client=chrome&q={keyword}&hl=en"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    suggestions = []
    try:
        response = requests.get(url, headers=headers, timeout=5)
        if response.status_code == 200:
            results = response.json()[1] # Google API returns [query, [suggestions], ...]
            # è¿‡æ»¤æ‰å¤ªçŸ­çš„æˆ–å®Œå…¨ä¸€æ ·çš„
            for res in results:
                if res != keyword and len(res) > len(keyword):
                    suggestions.append(res)
    except Exception as e:
        print(f"âŒ Error fetching Google suggestions: {e}")
        
    return suggestions[:5] # åªå–å‰5ä¸ªæœ€ç›¸å…³çš„

def fetch_reddit_questions(subreddits):
    """
    è·å– Reddit ä¸Šçš„çœŸå®ç”¨æˆ·æé—® (Content Angles)
    """
    print(f"ğŸ” [Reddit] Hunting for questions...")
    questions = []
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    for sub in subreddits:
        url = f"https://www.reddit.com/r/{sub}/new.json?limit=20" # çœ‹æœ€æ–°å¸–å­
        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                data = response.json()
                posts = data['data']['children']
                for post in posts:
                    title = html.unescape(post['data']['title'])
                    
                    # ç®€å•çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼šç­›é€‰åƒâ€œé—®é¢˜â€çš„å¸–å­
                    triggers = ["?", "help", "advice", "routine", "vs", "compare"]
                    if any(t in title.lower() for t in triggers):
                        questions.append({
                            'subreddit': sub,
                            'title': title,
                            'url': f"https://reddit.com{post['data']['permalink']}"
                        })
            time.sleep(1) 
        except Exception as e:
            print(f"âŒ Error fetching Reddit r/{sub}: {e}")
            
    # éšæœºæ‰“ä¹±å¹¶åªå–ä¸€éƒ¨åˆ†ï¼Œé¿å…æ¯æ¬¡éƒ½ä¸€æ ·
    random.shuffle(questions)
    return questions[:8]

def format_suggestion_to_todo(keyword, source_type):
    if source_type == "Google":
        # Google è¯é€šå¸¸æ˜¯çŸ­è¯­ï¼Œé€‚åˆåš H2 æˆ–æ–‡ç« ä¸»é¢˜
        return f"- [ ] **(Keyword)** `{keyword}` \n  - *Intent*: ğŸ” Search Query\n  - *Angle*: Ultimate Guide or Comparison"
    else:
        # Reddit æ˜¯å…·ä½“é—®é¢˜
        title = keyword['title']
        sub = keyword['subreddit']
        return f"- [ ] **(Question)** \"{title}\"\n  - *Source*: r/{sub}\n  - *Angle*: Answer this specific user pain point"

def update_blog_pool(google_data, reddit_data):
    script_dir = os.path.dirname(os.path.abspath(__file__))
    file_path = os.path.join(script_dir, TARGET_FILE)
    
    if not os.path.exists(file_path):
        print(f"âŒ Target file not found: {file_path}")
        return

    timestamp = datetime.now().strftime("%Y-%m-%d")
    
    print(f"ğŸ“ Writing to Blog Topic Pool...")
    
    with open(file_path, "a", encoding="utf-8") as f:
        f.write(f"\n\n## ğŸ•¸ï¸ Auto-Hunted Ideas ({timestamp})\n")
        f.write("> From Google Autocomplete & Reddit Discussions\n\n")
        
        if google_data:
            f.write("### ğŸ” Google Long-tail Keywords (SEO Gold)\n")
            for item in google_data:
                f.write(f"{format_suggestion_to_todo(item, 'Google')}\n")
        
        if reddit_data:
            f.write("\n### ğŸ—£ï¸ Reddit User Questions (Content Angles)\n")
            for item in reddit_data:
                f.write(f"{format_suggestion_to_todo(item, 'Reddit')}\n")

def main():
    print("--- Depology SEO Topic Hunter ---")
    
    # 1. Google æŒ–æ˜
    all_suggestions = []
    for seed in SEED_KEYWORDS:
        suggs = fetch_google_suggestions(seed)
        all_suggestions.extend(suggs)
        time.sleep(0.5)
    
    # éšæœºé€‰ä¸€äº›å±•ç¤ºï¼Œä¸è¦å¤ªå¤š
    selected_suggestions = random.sample(all_suggestions, min(len(all_suggestions), 8))
    
    # 2. Reddit æŒ–æ˜
    reddit_questions = fetch_reddit_questions(SUBREDDITS)
    
    # 3. å†™å…¥æ–‡ä»¶
    if selected_suggestions or reddit_questions:
        update_blog_pool(selected_suggestions, reddit_questions)
        print("\nâœ… Done! New topics added to 'Shopify SEO Blog/Blog_Topic_Pool.md'")
    else:
        print("\nâš ï¸ No new topics found.")

if __name__ == "__main__":
    main()
